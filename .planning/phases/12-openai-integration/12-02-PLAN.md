---
phase: 12-openai-integration
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - app/services/pipeline-executor.server.ts
autonomous: false

must_haves:
  truths:
    - "Pipeline executor loads OpenAI provider when model is gpt-*"
    - "OpenAI models can execute pipeline steps"
    - "Unsupported tools don't crash execution"
  artifacts:
    - path: "app/services/pipeline-executor.server.ts"
      provides: "OpenAI provider import for side-effect registration"
      contains: "providers/openai"
  key_links:
    - from: "app/services/pipeline-executor.server.ts"
      to: "app/lib/providers/openai.ts"
      via: "side-effect import"
      pattern: "import.*providers/openai"
---

<objective>
Wire OpenAI provider into pipeline executor and verify with real API call.

Purpose: Ensure OpenAI models actually work in pipeline execution
Output: Verified OpenAI provider integration with real API test
</objective>

<execution_context>
@/Users/brendan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brendan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-openai-integration/12-01-SUMMARY.md

@app/services/pipeline-executor.server.ts
@app/lib/providers/openai.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add OpenAI provider import to pipeline executor</name>
  <files>app/services/pipeline-executor.server.ts</files>
  <action>
Add side-effect import for OpenAI provider registration, alongside existing Anthropic import:

```typescript
// Import provider abstraction layer - registers providers on import
import "~/lib/providers/anthropic";
import "~/lib/providers/openai";
```

This ensures OpenAI provider factory is registered when pipeline-executor is loaded.
  </action>
  <verify>
- `grep "providers/openai" app/services/pipeline-executor.server.ts` finds the import
- `npx tsc --noEmit` passes
  </verify>
  <done>Pipeline executor imports OpenAI provider for side-effect registration</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>OpenAI provider integration with pipeline executor</what-built>
  <how-to-verify>
Test the OpenAI provider with a real API call:

1. Start the dev server: `npm run dev`

2. Create a test agent using an OpenAI model:
   - Go to Agents page
   - Create new agent with simple instructions: "Summarize the input in one sentence"
   - Select "GPT-4o" or "GPT-4o Mini" as the model
   - Note: You'll need to have an OpenAI API key configured (Phase 13 adds multi-key UI, but for now you can test by temporarily modifying the code to use a hardcoded key, or wait until Phase 13)

3. Alternative: Quick code test
   If you have an OpenAI API key, you can test directly in Node REPL:
   ```bash
   npx tsx -e "
   import { OpenAIProvider } from './app/lib/providers/openai';
   const p = new OpenAIProvider('your-key-here');
   p.chat([{role: 'user', content: 'Say hello'}], {model: 'gpt-4o-mini'})
     .then(r => console.log('Success:', r.content.slice(0,100)))
     .catch(e => console.error('Error:', e.message));
   "
   ```

4. Verify expected behavior:
   - Provider completes chat request without errors
   - Response has content and usage stats
   - No crashes from unsupported tools

If you don't have an OpenAI API key yet, you can approve this checkpoint to proceed. The provider integration is correct structurally - live testing can happen when Phase 13 adds multi-provider key storage.
  </how-to-verify>
  <resume-signal>Type "approved" if OpenAI provider works (or if deferring live test to Phase 13), or describe any issues</resume-signal>
</task>

</tasks>

<verification>
```bash
# Provider import present
grep "providers/openai" app/services/pipeline-executor.server.ts

# TypeScript compiles
npx tsc --noEmit

# All provider files exist
ls app/lib/providers/*.ts
```
</verification>

<success_criteria>
- Pipeline executor imports OpenAI provider for registration
- OpenAI models can be selected in registry (getProviderForModel returns "openai")
- Provider factory is registered and can create instances
- (Checkpoint) Real API call succeeds OR deferred to Phase 13 key storage
</success_criteria>

<output>
After completion, create `.planning/phases/12-openai-integration/12-02-SUMMARY.md`
</output>
