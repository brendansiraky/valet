---
phase: 12-openai-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - app/lib/models.ts
  - app/lib/providers/openai.ts
  - app/lib/providers/registry.ts
autonomous: true

must_haves:
  truths:
    - "OpenAI provider can complete a chat request"
    - "OpenAI models appear in ALL_MODELS list"
    - "Registry routes gpt-* models to openai provider"
    - "Provider gracefully handles unsupported tools (web_fetch, web_search)"
  artifacts:
    - path: "app/lib/providers/openai.ts"
      provides: "OpenAI provider implementation"
      exports: ["OpenAIProvider"]
    - path: "app/lib/models.ts"
      provides: "OPENAI_MODELS constant"
      contains: "OPENAI_MODELS"
  key_links:
    - from: "app/lib/providers/openai.ts"
      to: "app/lib/providers/registry.ts"
      via: "registerProviderFactory call"
      pattern: "registerProviderFactory.*openai"
    - from: "app/lib/providers/registry.ts"
      to: "openai provider"
      via: "getProviderForModel detection"
      pattern: "gpt-.*openai"
---

<objective>
Implement OpenAI provider with Chat Completions API, self-registration, and model detection.

Purpose: Enable pipelines to use OpenAI models (GPT-5.2, GPT-4o, etc.) alongside Anthropic models
Output: Working OpenAI provider that integrates with existing abstraction layer
</objective>

<execution_context>
@/Users/brendan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brendan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-openai-integration/12-RESEARCH.md

# Reference implementation
@app/lib/providers/anthropic.ts
@app/lib/providers/types.ts
@app/lib/providers/registry.ts
@app/lib/models.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install OpenAI SDK and add model definitions</name>
  <files>package.json, app/lib/models.ts</files>
  <action>
1. Install OpenAI SDK:
   ```bash
   npm install openai
   ```

2. Add OPENAI_MODELS to app/lib/models.ts:
   ```typescript
   export const OPENAI_MODELS = [
     { id: "gpt-4o", name: "GPT-4o", provider: "openai" },
     { id: "gpt-4o-mini", name: "GPT-4o Mini", provider: "openai" },
   ] as const;
   ```

3. Update ALL_MODELS to include OpenAI:
   ```typescript
   export const ALL_MODELS = [...ANTHROPIC_MODELS, ...OPENAI_MODELS] as const;
   ```

Note: Start with GPT-4o models (widely available). GPT-5.x can be added later when user has access.
  </action>
  <verify>
- `npm ls openai` shows package installed
- `grep "OPENAI_MODELS" app/lib/models.ts` finds the constant
- TypeScript compiles: `npx tsc --noEmit`
  </verify>
  <done>OpenAI SDK installed, OPENAI_MODELS defined, ALL_MODELS includes both providers</done>
</task>

<task type="auto">
  <name>Task 2: Create OpenAI provider and update registry</name>
  <files>app/lib/providers/openai.ts, app/lib/providers/registry.ts</files>
  <action>
1. Create app/lib/providers/openai.ts following anthropic.ts pattern:
   - Import OpenAI from "openai"
   - Import OPENAI_MODELS from ~/lib/models
   - Implement AIProvider interface
   - Constructor takes apiKey, creates OpenAI client
   - chat() method:
     - Map ChatMessage[] to OpenAI format (role stays same, system supported)
     - Call client.chat.completions.create()
     - Map response to ChatResult format (prompt_tokens -> inputTokens, etc.)
     - Handle tools parameter: log warning and skip unsupported tools (web_search, web_fetch)
     - Return empty citations array (Chat Completions doesn't have built-in citations)
   - validateKey() method:
     - Create test client with apiKey
     - Call client.models.list() (lightweight validation)
     - Return true on success, false on error
   - getModels() returns OPENAI_MODELS mapped to ProviderModel[]
   - Self-register factory at bottom: registerProviderFactory("openai", ...)

2. Update app/lib/providers/registry.ts getProviderForModel():
   - Uncomment/add OpenAI detection:
     ```typescript
     if (modelId.startsWith("gpt-") || modelId.startsWith("o3-") || modelId.startsWith("o4-")) {
       return "openai";
     }
     ```

Key implementation notes from research:
- Use `tools` array format, NOT deprecated `functions` parameter
- Check response.choices[0].message.content for text
- Check response.usage.prompt_tokens and completion_tokens for usage
- Catch AuthenticationError, RateLimitError, APIError for proper error handling
  </action>
  <verify>
- `npx tsc --noEmit` passes (no TypeScript errors)
- `grep "registerProviderFactory.*openai" app/lib/providers/openai.ts` finds self-registration
- `grep "gpt-.*openai" app/lib/providers/registry.ts` finds model detection
  </verify>
  <done>OpenAI provider implements AIProvider, self-registers, registry detects gpt-* models</done>
</task>

</tasks>

<verification>
```bash
# TypeScript compiles
npx tsc --noEmit

# OpenAI SDK installed
npm ls openai

# Provider exports exist
grep -l "OpenAIProvider" app/lib/providers/openai.ts

# Registry updated
grep "gpt-" app/lib/providers/registry.ts

# Models include OpenAI
grep "OPENAI_MODELS" app/lib/models.ts
```
</verification>

<success_criteria>
- OpenAI SDK (v6.x) installed in package.json
- OPENAI_MODELS constant defined with GPT-4o models
- ALL_MODELS includes both Anthropic and OpenAI models
- OpenAIProvider class implements AIProvider interface
- Provider self-registers with factory pattern
- Registry routes gpt-*/o3-*/o4-* models to openai provider
- Provider logs warning for unsupported tools (doesn't throw)
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/12-openai-integration/12-01-SUMMARY.md`
</output>
